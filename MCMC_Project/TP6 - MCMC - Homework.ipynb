{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align='center'> Monte-Carlo (MCMC) - TP6 (Homework due April 28 23h59) </h1>\n",
    "<h4 align='right'><i> author: Hicham Janati </i></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Metropolis-Hastings vs Rejection sampling\n",
    "We would like to sample from the density function:\n",
    "$$ f(x) \\propto  x^4\\exp(- x^2) $$ \n",
    "Using the proposal: $g(x) \\propto \\exp(- \\frac{1}{2} x^2) $.\n",
    "\n",
    "1. Find an empirical bound M so as to apply rejection sampling using g to sample from f and visualize the domination of f by Mg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "f = lambda x: np.exp(- x ** 2) * x ** 4\n",
    "g = lambda x: np.exp(- 0.5 * x ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement the rejection sampling algorithm and visualize the empirical histogram against the density f. Use scipy.integration to estimate the normalizing constant of f for accurate visualization of the density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sampling():\n",
    "    \"\"\"Reject sampling algorithm for f with Gaussian proposal g.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement the random walk metropolis hastings algorithm with a Gaussian conditional probability and visualize the empirical histogram against the normalized density f/F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rw_metropolis_hastings(n, f=f, seed=None):\n",
    "    \"\"\"Random walk metropolis hastings algorithm for f with Gaussian proposal g.\n",
    "    n: int.\n",
    "        number of samples.\n",
    "    f: callable.\n",
    "        density we want to sample from.\n",
    "    seed: int (optional).\n",
    "        random seed initialization.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Gibbs sampling\n",
    "Gibbs sampling is a Markov Chain sampling algorithm used to sample from a multivariate random variable by sampling each component from its conditional distribution while the others components are fixed.\n",
    "\n",
    "We would like to sample from a multivariate Gaussian $(X_1, X_2)$ with density:\n",
    "$$p(x_1, x_2) = \\frac{1}{2\\pi\\sqrt{1 - \\rho^2}} \\exp\\left(- \\frac{1}{2(1 - \\rho^2)}(x_1^2 + x_2^2 - 2\\rho x_1 x_2)\\right) $$\n",
    "*1. Identify the parameters of the distribution and compute the conditional distributions of each component.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. Implement the associated Gibbs sampler where each $X_1, X_2$ are sampled sequentially from the conditional distributions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampler():\n",
    "    \"\"\"Gibbs sampler for (X_1, X_2).\"\"\"\n",
    "\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualize the contour plots of the target distribution and that of the Gibbs Sampler for different values of n. How does it behave with $\\rho$ ?\n",
    "\n",
    "First the target density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "rho = 0.5\n",
    "S = np.array([[1, rho], [rho, 1]])\n",
    "grid = np.linspace(-3, 3, 1000)\n",
    "x, y = np.meshgrid(grid, grid)\n",
    "pos = np.empty((1000, 1000, 2))\n",
    "pos[:, :, 0] = x\n",
    "pos[:, :, 1] = y\n",
    "Z_true = multivariate_normal.pdf(pos, mean=[0, 0], cov=S)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.contour(x, y, Z_true, levels=np.logspace(-2, 0., 20) * Z_true.max())\n",
    "plt.title(\"True contour plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the contour plots as the Markov chain moves. Keep in mind that the Markov chain samples are correlated and therefore not iid ! We need to wait a whiler until the MC reaches the stationary distribution. We show this in the following plots:\n",
    "First we plot everything till a certain iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using the numpy corrcoef function, plot the auto-correlation values as a function of the lag: $corr(X_t, X_{t - lag})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Using what you observe, propose two arguments to add to your Gibbs sampler function to improve the simulation.\n",
    "What happens when $\\rho \\to 1$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Implement a Random Walk metropolis algorithm to simulate from p using a conditional proposal $\\mathcal N(0, \\tau I_2)$ and vary $\\tau$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rw_metropolis_hastings(n, f, tau=1., seed=None):\n",
    "    \"\"\"Random walk metropolis hastings algorithm for f with Gaussian proposal g.\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
